# MayStreet High Performance Query (HPQ) Springboard

## Welcome

Welcome to the High Performance Query (HPQ) springboard.

This springboard provides examples that can run inside Analytics Workbench and
retrieve data from out High Performance Query platform. This document provides
an overview of each of those examples in addition to an introduction to the HPQ
platform.

Feel free to copy the code from any of these examples into your own notebook, or
to modify the existing notebooks. This is an isolated copy of this springboard
which will remain in your own secured Analytics Workbench file system so you can
tinker, continue, or start over whenever and however you like!

If you'd like to make a copy of any notebook with a different name simply select
`File` -> `Save As...` and choose a new name.

## High Performance Query (HPQ)

MayStreet's High Performance Query platform is a low latency, near time market
data query API which supports high fidelity market data, low latencies, and high
throughput.

The core of the platform is a streaming WebSockets API which enables clients to
process arbitrarily-sized result sets.

To match the streaming nature of the API all notebooks in this springboard
process data iteratively without buffering the entire response. The WebSocket
message containing the complete response is read and processed frame-by-frame to
avoid buffering and the JSON contained therein is processed element-by-element
to avoid buffering the entire top level array.

## Notebooks

### _[combo-group-id.ipynb](combo-group-id.ipynb)_

SGX flags trades with a "Combo Group ID" which they describe in their
specification as follows:

>Used to group Combination Order Book executions and the trades in the
>constituent Order Books together

Despite this field being not necessarily having an equivalent for other
feeds/exchanges it is supplied by the HPQ API (as "combo_group_id") when
querying SGX data. This example uses this field to find the outright executions
which correspond to the first spread execution it is able to find on a certain
date.

### _[error.ipynb](error.ipynb)_

The HPQ API includes an endpoint whose only purpose is to fail for testing
purposes. This example leverages this endpoint to demonstrate the various
failure modalities which can be expected when interacting with the HPQ API.

### _[graph-trades.ipynb](graph-trades.ipynb)_

Downloads an entire day (2022-09-16) of iMpact trades for the spread product
`SB  FMV0022-SB  FMH0023` and plots the price thereof.

### _[metadata.ipynb](metadata.ipynb)_

Two means of inspecting product metadata are provided by the HPQ API:

- A listing of all products (and optionally their metadata)
- Snapshots of metadata for a particular product

This example demonstrates both of the above.

### _[no-follow.ipynb](no-follow.ipynb)_

Since HPQ is a near time market solution queries are accepted for the current
trading day. As the exchange sends information and it is received and processed
by the HPQ system it is made available as soon as technically and legally
possible. This formulation implies that there may be queries which cannot be
fully satisfied at some point in time but which may be satisfiable in the
future.

This example demonstrates how the HPQ API handles the above scenario (by
transmitting a `no_follow` record).

### _[pagination.ipynb](pagination.ipynb)_

Traditional APIs include parameters which can be used to:

- Limit response size
- Request that a response begin at some offset from the start of the result set
- Specify the ordering of returned data

HPQ supports none of these. All data returned by the HPQ API is in temporal
order. Where data is being returned for the same instant in time (for example
when obtaining a snapshot of the state of the market) it is unordered (you are
not guaranteed that the order is coherent or consistent).

The above formulation may seem limiting especially given the paginated manner in
which large result sets are typically queried and presented. But HPQ is a
streaming API and will stream you data as long as you like. If you stop reading
data and allow the TCP window to close, or close the connection the engine no
longer generates data. Rather than forcing users to specify the size of their
response in advance HPQ allows them to read as much as they like and then
abandon the response. This approach is less, not more, limiting than traditional
pagination.

This example demonstrates the above principle.

### _[post-trade.ipynb](post-trade.ipynb)_

Reads a CSV description of several trades and outputs post trade information for
each.

### _[sip-enrich-trades.ipynb](sip-enrich-trades.ipynb)_

This is an example shows how to query for BBOs and NBBO at the occurence of a trade

### _[sip-opening-price.ipynb](sip-opening-price.ipynb)_

An example that shows how to calculate the opening price when the end-of-day
statistics haven't (yet) been published by the exchange

### _[sip-query-by-venues.ipynb](sip-query-by-venues.ipynb)_

An example that shows querying SIP data by venues or a combination of venues.

### _[trade-conditions.ipynb](trade-conditions.ipynb)_

Annotates CME trades using a popular trade condition convention (TSUM, AB, AS,
and OR).

#### See Also

The techniques demonstrated by this example dovetail with those in the example
for [unreliable connections](unreliable.ipynb).

### _[transactions.ipynb](transactions.ipynb)_

Certain market data feeds have the concept of a transaction: Several updates
which are intended to be regarded atomically. CME is one of these. To support
this HPQ tags each update with a transaction ID. This number is somewhat
arbitrary and only has the meaning that for updates with the same transaction ID
they are part of the same transaction.

If receiving and processing each and every update is the desired use case then
one may disregard transactions. If however one only wishes to examine valid,
consistent states of the market one must only process updates after aggregating
across each transaction. The latter of these is the purpose of this example.

### _[unreliable.ipynb](unreliable.ipynb)_

The HPQ API supports extremely large result sets. As a large result set is
transferred the cost of losing that connection increases. In certain use cases
connections may be unreliable (i.e. liable to drop before the end of the
response). To use the HPQ API in these situations it is important that
techniques are available and used to transfer large result sets even if the
underlying connection is unreliable. This example demonstrates exactly these
techniques.

## Python Module

The [`hpq`](hpq.py) module provides utilities for interoperating with the HPQ
API and is used throughout the provided examples. For documentation use the
Python `help` function.
